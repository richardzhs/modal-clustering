---
title: "Modal Clustering"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    theme: united
    toc: yes
---


## Introduction
This report experiments a few density-based modal clustering methods and evaluate their performance on real datasets. We consider 3 methods: (1) Optimization on kernel density estimation (2) MeanShift algorithm (3) Gaussian mixture model. We use three labeled datasets (olive oil data, wine data, and 2d gaussian clusters) which we know the group labels in the data beforehand. We will test each of the methods on the two datasets and see what clusters we obtain. If the methods work, then the clusters should correspond to each group in the dataset.


## Loading Library and Packages
```{r}
library(meanShiftR)
library(mvtnorm)
library(here)
library(dplyr)
library(ggplot2)
```

```{r}
dir <- here()
```
If this doesn't work, can also use getwd() below which gets the current work directory.

```{r}
dir <- getwd()
```

```{r}
dir.sep <- "/"
echo <- F # controls whether outputs sequences during optimization
source(paste(dir, "function_packages", "kernel-density-estimation-functions.R", sep = dir.sep))
source(paste(dir, "function_packages", "gaussian-mixture-model-functions.R", sep = dir.sep))
source(paste(dir, "function_packages", "density-optimizing-functions.R", sep = dir.sep))
```


## Importing Data

### Olive Oil dataset
Olive oil data has 572 observations with 8 variables and 9 distinct groups.

```{r}
data.filename <- "olive.R"
data.pathname <- paste(dir, "data", data.filename, sep = dir.sep)
source(data.pathname)
```

A glimpse of olive oil data. columns represent variables and rows represent observations.
```{r}
oil.data <- X # save the matrix as 'oil.data'
oil.group.id <- group.id # save the id as 'oil.group.id'
sphered.oil <- sphere(oil.data)
head(X) 
```
Corresponding group id for olive oil data.
```{r}
oil.group.id
```

### Wine dataset
Wine data has 178 observations with 13 variables and 3 distinct groups. Details of the dataset can be found at: https://rdrr.io/cran/rattle.data/man/wine.html

```{r}
data(wine, package='rattle')
wine.matrix <- data.matrix(wine) # transform dataframe into matrix
wine.data <- wine.matrix[,-1] # data without group id
wine.group.id <- wine.matrix[,1] # group id
sphered.wine <- sphere(wine.data)
```

A glimpse of wine data.
```{r}
head(wine.matrix)
```
Corresponding group id for wine data
```{r}
wine.group.id
```

### 2d Gaussian clusters
Synthetic 2d dataset with 5000 observations and 15 distinct groups. Each group corresponds to a Gaussian cluster. The data is retrieved from: http://cs.joensuu.fi/sipu/datasets/

```{r}
dat <- read.csv('data/s-originals/s1.csv', header = T, sep = "")
colnames(dat) <- c('f1', 'f2', 'group_id')
```

Visualization of the dataset:
```{r}
ggplot(dat, aes(x = f1, y = f2, label = group_id)) +
  geom_point(aes(colour = as.character(group_id)))
```

For the sake of demonstration, we took a sample of 500 observations from the dataset
```{r}
set.seed(1234)
dat_sample <- sample_n(dat, 500)
dat_sample <- dat_sample[order(dat_sample$group_id),] # order based on group id
gaussian.2d <- as.matrix(dat_sample[,c('f1','f2')])
sphered.gaussian.2d <- sphere(gaussian.2d)
```

Visualization of the sample:
```{r}
ggplot(dat_sample, aes(x = f1, y = f2, label = group_id)) +
  geom_point(aes(colour = as.character(group_id)))
```

## Kernel Density Estimation
The first approach is by kernel density estimation. The idea is that we first estimate the density function of the datasets by kernel density estimation. By default we use gaussian kernel as our kernel function. After we have the estimated density function, we want to perform optimization on each data point. Basically we start with each data point and shift it uphill until it converges. Then the set of all points that converge to the same mode will correspond to one estimated cluster.  

### **1. Olive Oil**

#### Bandwidth Selection
We first sphere the dataset and pick the bandwidth by cross-validation 
```{r}
trial.hs <- c(0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0)
cv.search.out <- cv.search(sphered.oil, trial.hs)
h.oil <- cv.search.out$opt.smopar
```

```{r}
h.oil
```

#### Kernel Density Estimator

Fit kernel density estimation on oil data
```{r}
density.oil <- make.gaussian.kernel.density.estimate(sphered.oil, h.oil)
```

#### Numerical Optimization

Perform numerical optimization on each data point
```{r}
optim.out.filename <- "/optimizing_result/oil-density-optima.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")
gradient <- make.kde.gradient(sphered.oil, h.oil)
optimize.density(sphered.oil, density.oil, optim.out.pathname, gradient = gradient, echo = echo)
```

Read optimization result. 0 means all points converge.
```{r}
optima <- read.optim(sphered.oil, optim.out.pathname)
```

#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height[1:60]
```
These are merge distances based on hierarchical clustering of the local maximas. There is a "jump" between 48 and 49 which suggests that there are 572-48=524 distinct groups, which is apparently not true.


### **2. Wine**
We repeat the procedure above for wine dataset.

#### Bandwidth Selection

```{r}
trial.hs <- c(0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0)
cv.search.out <- cv.search(sphered.wine, trial.hs)
h.wine <- cv.search.out$opt.smopar
```

```{r}
h.wine
```

#### Kernel Density Estimator

```{r}
density.wine <- make.gaussian.kernel.density.estimate(sphered.wine, h.wine)
```

#### Numerical Optimization

```{r}
optim.out.filename <- "/optimizing_result/wine-density-optima.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")


gradient <- make.kde.gradient(sphered.wine, h.wine)
optimize.density(sphered.wine, density.wine, optim.out.pathname, gradient = gradient, echo = echo)
```


```{r}
optima <- read.optim(sphered.wine, optim.out.pathname)
```


#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height
```
There is a significant "jump", which suggest there are 178 distinct groups, i.e. every observation forms a group itself.


### **3. 2d Gaussian Clusters**
Now the same procedure for 2d Gaussian clusters.

#### Bandwidth Selection

```{r}
trial.hs <- c(0.001, 0.1, 0.15, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0)
cv.search.out <- cv.search(sphered.gaussian.2d, trial.hs)
h.gaussian.2d <- cv.search.out$opt.smopar
```

```{r}
h.gaussian.2d
```

#### Kernel Density Estimator

```{r}
density.gaussian.2d <- make.gaussian.kernel.density.estimate(sphered.gaussian.2d, h.gaussian.2d)
```

#### Numerical Optimization

```{r}
optim.out.filename <- "/optimizing_result/gaussian-2d-density-optima-h=0.024-cv.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")


gradient <- make.kde.gradient(sphered.gaussian.2d, h.gaussian.2d)
optimize.density(sphered.gaussian.2d, density.gaussian.2d, optim.out.pathname, gradient = gradient, echo = echo)
```


```{r}
optima <- read.optim(sphered.gaussian.2d, optim.out.pathname)
```


#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height
```
There is a "jump" at merge 302, suggesting at there are 500-302+1=199 distinct groups.  
This is the result based on h = 0.024, the bandwidth obtained from cross validation. We now adjust the bandwith to be h = 0.11, that is the same as in mean shift. We want to see how many modes we get in this case.

```{r}
h = 0.11
density.gaussian.2d <- make.gaussian.kernel.density.estimate(sphered.gaussian.2d, h)
```

```{r}
optim.out.filename <- "/optimizing_result/gaussian-2d-density-optima-h=0.11.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")


gradient <- make.kde.gradient(sphered.gaussian.2d, h)
optimize.density(sphered.gaussian.2d, density.gaussian.2d, optim.out.pathname, gradient = gradient, echo = echo)
```

```{r}
optima <- read.optim(sphered.gaussian.2d, optim.out.pathname)
```

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height
```
There are 14 "large" merge distances at the end, which suggests that there are 15 groups. This is consistent with what we expect and is the same as in meanshift algorithm.

## MeanShift Algorithm
The second approach is mean-shift algorithm. The idea is to iteratively shift the each point to the weighted average of the density within its neighbourhood until it converges. The weight is determined by the kernel function and the bandwidth h. Here we also use Gaussian kernel by default. The set of all points that converge to the same mode belong to one group.  
By default we will use the bandwidth computed from cross validation. If the number of distinct groups isn't correct, we will try to adjust the bandwidth so that it matches the true number of groups.

### **1. Olive Oil**

#### Bandwith picked by cross-validation
```{r}
ms <- meanShift(sphered.oil, bandwidth = rep(h.oil, 8), iterations = 1000000)
max(ms$assignment)
```
There are 524 groups after applying meanshift algorithm which behaves poorly. The result is exactly the same as the KDE optimization approach.

#### Bandwidth matched to the true number of groups
```{r}
ms <- meanShift(sphered.oil, bandwidth = rep(0.96, 8), iterations = 1000000)
max(ms$assignment)
```
See if it matches the true labels. Here is the corresponding assignment:
```{r}
as.vector(ms$assignment)
```
True labels:
```{r}
oil.group.id
```
Clearly the assignments are off.


### **2. Wine**
#### Bandwith picked by cross-validation
```{r}
ms <- meanShift(sphered.wine, bandwidth = rep(h.wine, 13), iterations = 1000000)
max(ms$assignment)
```
There are 178 groups after applying meanshift algorithm which again it is exactly the same as the KDE optimization approach.
#### Bandwidth matched to the true number of groups
```{r}
ms <- meanShift(sphered.wine, bandwidth = rep(1.565, 13), iterations = 1000000)
max(ms$assignment)
```
Corresponding assignments:
```{r}
as.vector(ms$assignment)
```
True labels:
```{r}
wine.group.id
```
Again, the assignments are off.


### **3. 2d Gaussian Clusters**
#### Bandwith picked by cross-validation
```{r}
ms <- meanShift(sphered.gaussian.2d, bandwidth = rep(h.gaussian.2d, 2), iterations = 1000000)
max(ms$assignment)
```
#### Bandwidth matched to the true number of groups
```{r}
ms <- meanShift(sphered.gaussian.2d, bandwidth = rep(0.11, 2), iterations = 1000000)
max(ms$assignment)
```
Corresponding assignments:
```{r}
as.vector(ms$assignment)
```
True labels:
```{r}
as.vector(dat_sample$group_id)
```
The assignments match the true labels this time (group ids are not exactly the same but they represent the same groups).

## Gaussian Mixture
The previous two approaches all behave poorly in the way that overestimates the number of modes in each of dataset. This suggests that many of the modes computed in both way do not correspond to the true modes but just some noise from data. One way to reduce such noise can be increasing the bandwidth. This will generally make the estimated density smoother. Before we do that, we first want to verify that given a smooth estimate, are we going to get the right result?

To do that, assume that we know the groups for each dataset beforehand. Then we fit the gaussian mixture model to data using known group labels. After we have the gaussian mixture estimate, we run the optimization again and see the results.

### **1. Olive Oil**

#### Fit Mixture Model

```{r}
mix.par.oil <- fit.gmm(oil.data, oil.group.id)
mix.dens.oil <- make.dgmm(mix.par.oil)
```

#### Numerical Optimization On Mixture Model

```{r}
optim.out.filename <- "/optimizing_result/olive-mixture-density-optima.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")

control <- control <- list("maxit" = 10000, "fnscale" = -1e-20)
gradient <- make.dgmm.gradient(mix.par.oil)

optimize.density(oil.data, mix.dens.oil, optim.out.pathname, gradient = gradient, control = control, echo = echo)
```


```{r}
optima <- read.optim(oil.data, optim.out.pathname)
```
#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height[550:571]
```
There are 8 "large" merge distances at the end, which suggests that there are 9 groups. This is consistent with what we expect.


### **2. Wine**

#### Fit mixture model

```{r}
mix.par.wine <- fit.gmm(wine.data, wine.group.id)
mix.dens.wine <- make.dgmm(mix.par.wine)
```

#### Numerical Optimization

```{r}
optim.out.filename <- "/optimizing_result/wine-mixture-density-optima.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")

control <- list("maxit" = 10000, "fnscale" = -1e-20)

gradient <- make.dgmm.gradient(mix.par.wine)
optimize.density(wine.data, mix.dens.wine, optim.out.pathname, gradient = gradient, control = control)
```


```{r}
optima <- read.optim(wine.data, optim.out.pathname)
```
#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height
```
There is a "jump" at the second last merge which suggest that there are 3 groups. This is consistent with what we expect.

### **3. 2d Gaussian Clusters**

#### Fit Mixture Model

```{r}
mix.par.gaussian.2d <- fit.gmm(gaussian.2d, dat_sample$group_id)
mix.dens.gaussian.2d <- make.dgmm(mix.par.gaussian.2d)
```

#### Numerical Optimization On Mixture Model

```{r}
optim.out.filename <- "/optimizing_result/gaussian-2d-mixture-density-optima.R"
optim.out.pathname <- paste(dir, optim.out.filename, sep = "\\")

control <- control <- list("maxit" = 10000, "fnscale" = -1e-20)
gradient <- make.dgmm.gradient(mix.par.gaussian.2d)

optimize.density(gaussian.2d, mix.dens.gaussian.2d, optim.out.pathname, gradient = gradient, control = control, echo = echo)
```


```{r}
optima <- read.optim(gaussian.2d, optim.out.pathname)
```

#### Cluster Local Maxima

```{r}
clust <- hclust(dist(optima), method = "ward.D2")
clust$height
```
There are 14 "large" merge distances at the end, which suggests that there are 15 groups. This is consistent with what we expect.

## Discussion
So far based on the experiments on olive oil dataset and wine dataset, neither of KDE method or mean shift algorithm performed well on clustering. They both have overwhelmingly many modes which suggest that the fitted models are not smooth. This means we might want to decrease the variance at each point in order to make the KDE smoother. A natural way is to increase the bandwidth h. In this case, variance decreases and bias increases. But we don't care about the bias too much because we don't need the accurate estimate for each point. We only need to know the locations maximas in order to identify the "distinct" modes.  
So far we have tried manually increasing the bandwidths and it did decrease the number of modes we get (have not yet included in this report). However, we haven't obtained the h such that it produces the consistent result as we expect. One reason is that as h increases, it takes much more iterations and much longer time for optimization to converge. This might be because KDE has small curvature when h is large. Therefore, each iteration takes only a small amount and step which results in a slower convergence rate.   
On the other hand, the Gaussian mixture models work pretty well for both datasets which should make sense. Because we put the Gaussian density centered at each of the groups in advance, the modes of each Gaussian density should be comparably larger than other regions in the mixture.  
Also, one interesting fact is that KDE and mean-shift algorithm works out to be exactly the same which might suggest that the two methods are the same implicitly.